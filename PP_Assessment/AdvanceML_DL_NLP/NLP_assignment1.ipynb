{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_wZgXQ79fvyB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from googleapiclient.discovery import build\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your YouTube Data API credentials\n",
        "API_KEY = 'AIzaSyCc2L-xpOHzPlZG9x2OWW9LTf4HdudTx2A'\n",
        "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "YOUTUBE_API_VERSION = 'v3'\n"
      ],
      "metadata": {
        "id": "TfmKF5Y0lXCJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_comments(video_id):\n",
        "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "    comments = []\n",
        "\n",
        "    # Retrieve video comments\n",
        "    response = youtube.commentThreads().list(\n",
        "        part='snippet',\n",
        "        videoId=video_id,\n",
        "        textFormat='plainText',\n",
        "        maxResults=100\n",
        "    ).execute()\n",
        "\n",
        "    while response:\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        # Check if there are more comments\n",
        "        if 'nextPageToken' in response:\n",
        "            response = youtube.commentThreads().list(\n",
        "                part='snippet',\n",
        "                videoId=video_id,\n",
        "                textFormat='plainText',\n",
        "                maxResults=100,\n",
        "                pageToken=response['nextPageToken']\n",
        "            ).execute()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n"
      ],
      "metadata": {
        "id": "QIY6YCAyk8sr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_comments(comments):\n",
        "    # Download stopwords\n",
        "    nltk.download('stopwords')\n",
        "    \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_comments = []\n",
        "\n",
        "    for comment in comments:\n",
        "        # Remove punctuation and convert to lowercase\n",
        "        comment = ''.join([char for char in comment if char.isalnum() or char.isspace()]).lower()\n",
        "        # Remove stopwords\n",
        "        comment = ' '.join([word for word in comment.split() if word not in stop_words])\n",
        "        cleaned_comments.append(comment)\n",
        "\n",
        "    return cleaned_comments\n"
      ],
      "metadata": {
        "id": "XqaZf91KlFms"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_demanding_topic(comments):\n",
        "    word_counts = {}\n",
        "\n",
        "    # Count occurrences of words\n",
        "    for comment in comments:\n",
        "        for word in comment.split():\n",
        "            if word in word_counts:\n",
        "                word_counts[word] += 1\n",
        "            else:\n",
        "                word_counts[word] = 1\n",
        "\n",
        "    # Sort words by their counts in descending order\n",
        "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the most demanding topic\n",
        "    return sorted_words[0][0]\n"
      ],
      "metadata": {
        "id": "SX69_WtAlL34"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Prompt the user to enter a YouTube video URL\n",
        "    video_url = input('Enter the YouTube video URL: ')\n",
        "\n",
        "    # Extract video ID from the URL\n",
        "    video_id = 'fKJVxItLiUw'\n",
        "\n",
        "    # Get comments from the video\n",
        "    comments = get_video_comments(video_id)\n",
        "\n",
        "    # Clean the comments\n",
        "    cleaned_comments = clean_comments(comments)\n",
        "\n",
        "    # Determine the most demanding topic\n",
        "    most_demanding_topic = determine_demanding_topic(cleaned_comments)\n",
        "\n",
        "    # Print the most demanding topic\n",
        "    print('The most demanding topic in the video comments is:', most_demanding_topic)\n",
        "\n",
        "    # Store the comments in a CSV file\n",
        "    df = pd.DataFrame({'Comments': comments})\n",
        "    df.to_csv('video_comments.csv', index=False, quoting=csv.QUOTE_ALL)\n",
        "    print('Comments saved to video_comments.csv')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMYOWa3NoBT_",
        "outputId": "2895b1f1-85fc-401e-fdd8-6ecaacc0cacc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://www.youtube.com/watch?v=fKJVxItLiUw&list=RDCMUCWX0cUR2rZcqKei1Vstww-A&start_radio=1\n",
            "The most demanding topic in the video comments is: course\n",
            "Comments saved to video_comments.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    }
  ]
}