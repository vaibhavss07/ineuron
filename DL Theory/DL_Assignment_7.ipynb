{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8",
      "metadata": {
        "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8"
      },
      "source": [
        "# Assignment 7 Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa14923c-b8ac-47cd-b839-a36cd92889b0",
      "metadata": {
        "id": "aa14923c-b8ac-47cd-b839-a36cd92889b0"
      },
      "source": [
        "**1. Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN, and a vector-to-sequence RNN?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f46002-63bd-41e7-8a2b-dea586ec93d6",
      "metadata": {
        "id": "f4f46002-63bd-41e7-8a2b-dea586ec93d6"
      },
      "source": [
        "**Ans:** Here are a few examples of applications for each type of RNN:\n",
        "\n",
        "1. `Sequence-to-sequence RNN`:\n",
        "\n",
        "    1. Machine translation: translating a sequence of words from one language to another.\n",
        "\n",
        "    2. Speech recognition: transcribing spoken words into a sequence of written words.\n",
        "\n",
        "    3. Chatbot: generating a sequence of responses to a sequence of inputs from a user.\n",
        "\n",
        "    4. Music generation: generating a sequence of musical notes based on a given style or genre.\n",
        "\n",
        "2. `Sequence-to-vector RNN`:\n",
        "\n",
        "    1. Sentiment analysis: classifying a sequence of words as positive or negative sentiment.\n",
        "\n",
        "    2. Stock prediction: predicting the stock price based on a sequence of historical stock prices.\n",
        "\n",
        "    3. Video classification: classifying a sequence of frames in a video into different categories (e.g., action, comedy, drama).\n",
        "\n",
        "    4. Natural language generation: generating a summary or description of a sequence of events or information.\n",
        "\n",
        "3. `Vector-to-sequence RNN`:\n",
        "\n",
        "    1. Image captioning: generating a sequence of words describing the content of an image.\n",
        "\n",
        "    2. Speech synthesis: generating a sequence of spoken words based on a given text input.\n",
        "\n",
        "    3. Music transcription: transcribing a sequence of musical notes into sheet music.\n",
        "\n",
        "    4. Language translation: translating a single word or sentence into a sequence of words in another language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7d4b67-eb90-48da-8bb0-88276173c8c7",
      "metadata": {
        "id": "db7d4b67-eb90-48da-8bb0-88276173c8c7"
      },
      "source": [
        "**2. How many dimensions must the inputs of an RNN layer have? What does each dimension represent? What about its outputs?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7efbcb6a-f9be-40ee-921e-74c7b0537388",
      "metadata": {
        "id": "7efbcb6a-f9be-40ee-921e-74c7b0537388"
      },
      "source": [
        "**Ans:** The inputs of an RNN layer typically have three dimensions: batch size, time steps, and input features.\n",
        "\n",
        "* Batch size represents the number of samples that are processed in a single batch.\n",
        "* Time steps represent the number of time steps in the sequence. For example, if the input is a sentence with 10 words, and we use one word as a time step, then the time steps would be 10.\n",
        "* Input features represent the number of features in each time step. For example, if each word in the sentence is represented as a one-hot encoded vector of length 10000, then the input features would be 10000.\n",
        "\n",
        "The outputs of an RNN layer also have three dimensions, which are typically batch size, time steps, and hidden size.\n",
        "\n",
        "* Batch size is the same as in the input.\n",
        "* Time steps is also the same as in the input.\n",
        "* Hidden size represents the number of hidden units in the RNN layer.\n",
        "\n",
        "The output of each time step in the RNN layer is a hidden state vector, which is computed based on the input and the previous hidden state. The final output of the RNN layer can be obtained by combining the hidden state vectors from all time steps in the sequence. Depending on the specific architecture of the RNN, there may also be additional output dimensions, such as the number of output classes in a classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efda6b15-99d6-4b8e-8cf5-b388a38a9c3f",
      "metadata": {
        "id": "efda6b15-99d6-4b8e-8cf5-b388a38a9c3f"
      },
      "source": [
        "**3. If you want to build a deep sequence-to-sequence RNN, which RNN layers should have return_sequences=True? What about a sequence-to-vector RNN?**\n",
        "\n",
        "**Ans:** In a deep sequence-to-sequence RNN, all RNN layers except the last one should have `return_sequences=True`. This allows all hidden states in the sequence to be passed to the next RNN layer in the stack.\n",
        "\n",
        "In a sequence-to-vector RNN, only the last RNN layer should have `return_sequences=False` (which is the default value), since we are only interested in the final hidden state to generate the output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89995ac7-2ed0-489c-a79f-45e4f313e828",
      "metadata": {
        "id": "89995ac7-2ed0-489c-a79f-45e4f313e828"
      },
      "source": [
        "**4. Suppose you have a daily univariate time series, and you want to forecast the next seven days. Which RNN architecture should you use?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c247d3a4-40dd-4bad-bcb2-b9e05aed8421",
      "metadata": {
        "id": "c247d3a4-40dd-4bad-bcb2-b9e05aed8421"
      },
      "source": [
        "**Ans:** For forecasting the next seven days in a daily univariate time series, a suitable RNN architecture to use is the Sequence-to-Sequence (Seq2Seq) model with an encoder-decoder architecture.\n",
        "\n",
        "The encoder takes the input sequence (e.g., past 30 days) and outputs a fixed-length vector that summarizes the input information. The decoder then takes this vector and generates the output sequence (e.g., forecast for the next 7 days).\n",
        "\n",
        "To improve the performance of the model, we can use techniques such as teacher forcing, attention mechanism, and residual connections. Additionally, we can add other components to the model, such as a seasonal component, to capture the periodicity of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "978c7ccc-447e-49dd-8b38-303c21878753",
      "metadata": {
        "id": "978c7ccc-447e-49dd-8b38-303c21878753"
      },
      "source": [
        "**5. What are the main difficulties when training RNNs? How can you handle them?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1174f141-d95c-4a81-b040-e1fa55234f00",
      "metadata": {
        "id": "1174f141-d95c-4a81-b040-e1fa55234f00"
      },
      "source": [
        "**Ans:** There are several main difficulties when training RNNs, including:\n",
        "\n",
        "* `Vanishing and exploding gradients`: The gradients in RNNs can become too small or too large, making it difficult to learn long-term dependencies. This can be addressed by using techniques such as gradient clipping, weight initialization, and using gated RNN variants like LSTMs and GRUs.\n",
        "\n",
        "* `Overfitting`: RNNs can easily overfit due to their high capacity and the sequential nature of the data. Regularization techniques such as dropout, early stopping, and weight decay can help address this problem.\n",
        "\n",
        "* `Difficulty in parallelization`: Unlike feedforward neural networks, RNNs are difficult to parallelize due to their sequential nature. This can result in slow training times, especially for large sequences. One way to handle this is to use techniques such as truncated backpropagation through time (BPTT) and bucketing to reduce the length of the sequences.\n",
        "\n",
        "* `Input normalization`: RNNs are sensitive to the scale of the input features, and normalization can help improve their performance. Standard normalization techniques such as min-max scaling and z-score normalization can be used.\n",
        "\n",
        "* `Choosing hyperparameters`: RNNs have several hyperparameters such as the number of layers, hidden units, learning rate, and batch size that need to be tuned carefully. Techniques such as grid search, random search, and Bayesian optimization can be used to find optimal hyperparameters.\n",
        "\n",
        "To handle these difficulties, it is important to experiment with different architectures, regularization techniques, optimization algorithms, and hyperparameters to find the best combination for the given problem. It is also important to monitor the training process carefully, visualize the results, and make adjustments as necessary. Additionally, using pre-trained models or transfer learning can help to overcome some of these difficulties."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "694b97b0-82ba-433d-803a-e01e2b8fdcb6",
      "metadata": {
        "id": "694b97b0-82ba-433d-803a-e01e2b8fdcb6"
      },
      "source": [
        "**6. Can you sketch the LSTM cellâ€™s architecture?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd3d7ac-c944-4873-ada8-715d31b02ef6",
      "metadata": {
        "id": "cfd3d7ac-c944-4873-ada8-715d31b02ef6"
      },
      "source": [
        "**Ans:** Yes, I can sketch the LSTM cell's architecture."
      ]
    },
    {
      "cell_type": "raw",
      "id": "6ebae9ac-c5a6-4026-b67e-b103e7e0486f",
      "metadata": {
        "id": "6ebae9ac-c5a6-4026-b67e-b103e7e0486f"
      },
      "source": [
        "                   Output\n",
        "                      ^\n",
        "                      |\n",
        "                      v\n",
        "             +-------------------+\n",
        "             |                   |\n",
        "             |                   |\n",
        "             v                   |\n",
        "         +-----+                |\n",
        "Input-->| LSTM|------------+   |\n",
        "         +-----+            |   |\n",
        "             ^              v   |\n",
        "             |          +------+ |\n",
        "             +----------| tanh | |\n",
        "                        +------+ |\n",
        "                               v\n",
        "                             Gates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d53389b-7c48-4e2e-855e-50cab0ceec27",
      "metadata": {
        "id": "4d53389b-7c48-4e2e-855e-50cab0ceec27"
      },
      "source": [
        "The LSTM cell has three gates: the input gate, forget gate, and output gate, which control the flow of information into and out of the cell. It also has a memory cell that stores the previous hidden state and a current hidden state that is passed to the next cell in the sequence. The tanh activation function is applied to the current hidden state to introduce non-linearity into the cell. The input and forget gates are sigmoid layers that control the amount of information to be passed to the memory cell and the amount of information to be forgotten from the previous cell state. The output gate is also a sigmoid layer that controls the amount of information to be output from the current hidden state.\n",
        "\n",
        "The LSTM architecture enables it to learn long-term dependencies by allowing information to flow through the cell without being lost or distorted. The gates provide a way for the model to selectively remember or forget information, which makes it suitable for tasks such as speech recognition, machine translation, and sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2390b50-0108-462e-9673-da389adc3ac9",
      "metadata": {
        "id": "a2390b50-0108-462e-9673-da389adc3ac9"
      },
      "source": [
        "**7. Why would you want to use 1D convolutional layers in an RNN?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44fe98af-a3bc-4cdd-8ed1-f2c6bca9ef3d",
      "metadata": {
        "id": "44fe98af-a3bc-4cdd-8ed1-f2c6bca9ef3d"
      },
      "source": [
        "**Ans:** 1D convolutional layers can be used in an RNN to capture local patterns in sequential data. The convolution operation applies a sliding window over the sequence, performing a dot product between the weights of the filter and the corresponding input elements. This operation can extract local features, such as patterns or motifs, that are relevant to the task.\n",
        "\n",
        "One advantage of using 1D convolutional layers in an RNN is that it can reduce the dimensionality of the input, which can be useful for processing long sequences. This can help to reduce the computational complexity of the model and prevent overfitting.\n",
        "\n",
        "Another advantage is that the use of 1D convolutions can provide the model with a translational invariance property, which means that the model can identify patterns regardless of their position in the sequence. This can be useful for tasks such as speech recognition or music analysis, where the location of relevant features may vary.\n",
        "\n",
        "Overall, using 1D convolutional layers in an RNN can improve the model's ability to extract meaningful features from the input sequence and improve its performance on tasks that require the identification of local patterns or motifs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a4a348-767b-479a-8cf0-3b691e21d7ab",
      "metadata": {
        "id": "b7a4a348-767b-479a-8cf0-3b691e21d7ab"
      },
      "source": [
        "**8. Which neural network architecture could you use to classify videos?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f7fcdb4-57c6-49d5-b08b-7d143a80709a",
      "metadata": {
        "id": "2f7fcdb4-57c6-49d5-b08b-7d143a80709a"
      },
      "source": [
        "**Ans:** To classify videos, a suitable neural network architecture is the 3D Convolutional Neural Network (CNN).\n",
        "\n",
        "A 3D CNN operates on a sequence of video frames, treating them as 3D volumes with depth, height, and width. The network applies 3D convolutional filters to the video sequence, which allows the model to capture spatial and temporal features in the data.\n",
        "\n",
        "The 3D CNN architecture typically consists of multiple layers of 3D convolutional filters, followed by pooling and activation layers. The output of the convolutional layers is fed into fully connected layers, which output the final classification labels.\n",
        "\n",
        "One advantage of using a 3D CNN for video classification is that it can capture both spatial and temporal information in the video sequence. This can help the model to identify complex patterns in the video data and improve its classification accuracy.\n",
        "\n",
        "Another advantage is that 3D CNNs can be trained end-to-end, which means that the entire model can be optimized for the task without the need for hand-crafted features or pre-processing steps.\n",
        "\n",
        "Overall, the 3D CNN architecture is a powerful and effective approach for video classification tasks, and it has been successfully applied in areas such as action recognition, facial expression analysis, and object detection in videos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0256a26e-3cdc-4fc0-8ca1-9f636cb4aafb",
      "metadata": {
        "id": "0256a26e-3cdc-4fc0-8ca1-9f636cb4aafb"
      },
      "source": [
        "**9. Train a classification model for the SketchRNN dataset, available in TensorFlow Datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4Rh5yMvFgE_",
      "metadata": {
        "id": "e4Rh5yMvFgE_"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39GxCdO7E33c",
      "metadata": {
        "id": "39GxCdO7E33c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c6291e",
      "metadata": {
        "id": "b7c6291e"
      },
      "outputs": [],
      "source": [
        "# Load the quickdraw_bitmap split of the SketchRNN dataset\n",
        "data = tfds.load('quickdraw_bitmap', split='train', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0BJiueJIFANp",
      "metadata": {
        "id": "0BJiueJIFANp"
      },
      "outputs": [],
      "source": [
        "# Convert the dataset to numpy arrays\n",
        "data = tfds.as_numpy(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MuJhywswFFF9",
      "metadata": {
        "id": "MuJhywswFFF9"
      },
      "outputs": [],
      "source": [
        "# Extract the stroke sequences from the bitmap images\n",
        "inputs = []\n",
        "labels = []\n",
        "for image, label in data:\n",
        "    sequence = tfa.image.rotate(image, 90)  # Rotate the image to align with the SketchRNN format\n",
        "    sequence = tfa.image.flip_left_right(sequence)  # Flip the image to mirror the stroke directions\n",
        "    sequence = np.pad(sequence, ((1, 0), (0, 0), (0, 0)), mode='constant')[:-1]  # Add a starting pen-down stroke\n",
        "    inputs.append(sequence)\n",
        "    labels.append(label)\n",
        "\n",
        "inputs = np.array(inputs)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "arG6vT3HFLA2",
      "metadata": {
        "id": "arG6vT3HFLA2"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(None, 5)),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.Dense(345, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RxG2WMEaFNQb",
      "metadata": {
        "id": "RxG2WMEaFNQb"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S-epFucKFPps",
      "metadata": {
        "id": "S-epFucKFPps"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "model.fit(inputs, labels, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yhMld8PMFToR",
      "metadata": {
        "id": "yhMld8PMFToR"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "test_data = tfds.load('quickdraw_bitmap', split='test', as_supervised=True)\n",
        "test_data = tfds.as_numpy(test_data)\n",
        "test_inputs = []\n",
        "test_labels = []\n",
        "for image, label in test_data:\n",
        "    sequence = tfa.image.rotate(image, 90)\n",
        "    sequence = tfa.image.flip_left_right(sequence)\n",
        "    sequence = np.pad(sequence, ((1, 0), (0, 0), (0, 0)), mode='constant')[:-1]\n",
        "    test_inputs.append(sequence)\n",
        "    test_labels.append(label)\n",
        "\n",
        "test_inputs = np.array(test_inputs)\n",
        "test_labels = np.array(test_labels)\n",
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_labels, batch_size=32)\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}